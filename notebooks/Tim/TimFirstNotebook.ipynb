{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:30:16.240327Z",
     "start_time": "2025-05-09T12:30:16.236355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "#  Waldschäden‑Detection  |  XGBoost  +  Isotone  |  Jahr‑stratifizierte CV\n",
    "#  ---------------------------------------------------------------------------\n",
    "#  - nutzt PR‑AUC ('aucpr') als Optimierungskriterium\n",
    "#  - 5‑fach GroupKFold‑Cross‑Validation (Gruppen = Jahr)\n",
    "#  - ermittelt τ* (F2‑Optimum bei Recall ≥ 0.60) je Fold, nimmt Median\n",
    "# =============================================================================\n"
   ],
   "id": "66eb19512d2a697c",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:30:16.283520Z",
     "start_time": "2025-05-09T12:30:16.279538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# 1 │ Bibliotheken & Logging                                             │\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "import datetime\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (average_precision_score, balanced_accuracy_score,\n",
    "                             confusion_matrix, fbeta_score, f1_score,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Logging\n",
    "# -----------------------------------------------------------------------\n",
    "LOG_DIR = Path(\"Tim/logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "log_file = LOG_DIR / f\"run_{datetime.datetime.now():%Y%m%d_%H%M%S}.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "\n",
    "# Helper für optische Überschriften in Notebook‑Ausgabe\n",
    "def banner(text: str) -> None:\n",
    "    line = \"═\" * 78\n",
    "    print(f\"\\n{line}\\n{text}\\n{line}\")\n",
    "\n",
    "# Reproduzierbarkeit\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "logging.info(\"Notebook gestartet – Seeds gesetzt (42).\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:30:16.450508Z",
     "start_time": "2025-05-09T12:30:16.311492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# 2 │ Daten laden  &  Leakage‑Prüfung                                    │\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "banner(\"DATA LOADING\")\n",
    "\n",
    "DATA_DIR = Path(\"data\")                       # <‑‑ Pfad ggf. anpassen\n",
    "train_df = (\n",
    "    pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "      .sort_values([\"forest_id\", \"year\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "val_df = (\n",
    "    pd.read_csv(DATA_DIR / \"validation.csv\")\n",
    "      .sort_values([\"forest_id\", \"year\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# forest_id‑Leakage: gleiche Fläche darf nicht in beiden Splits sein\n",
    "# -----------------------------------------------------------------------\n",
    "overlap_ids = set(train_df.forest_id) & set(val_df.forest_id)\n",
    "if overlap_ids:\n",
    "    print(f\"⚠️  {len(overlap_ids)} forest_ids doppelt; werden aus Validation entfernt.\")\n",
    "    val_df = val_df[~val_df.forest_id.isin(overlap_ids)]\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Zeilen‑/Spalten‑Koordinaten (142 × 142 Raster) anlegen, falls nicht vorhanden\n",
    "# -----------------------------------------------------------------------\n",
    "for df in (train_df, val_df):\n",
    "    if {\"row\", \"col\"}.issubset(df.columns):\n",
    "        continue\n",
    "    n_rows = n_cols = 142\n",
    "    df[\"row\"] = df[\"forest_id\"] // n_cols\n",
    "    df[\"col\"] = df[\"forest_id\"] % n_cols\n"
   ],
   "id": "dfe1c757215853f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════════════\n",
      "DATA LOADING\n",
      "══════════════════════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:30:44.492042Z",
     "start_time": "2025-05-09T12:30:16.475429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# 3 │ Feature Engineering                                                │\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "banner(\"FEATURE ENGINEERING\")\n",
    "\n",
    "def engineer_features(group: pd.DataFrame, lags: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Erzeugt spektrale Indizes + zeitliche Ableitungen für eine Wald‑ID.\"\"\"\n",
    "    eps = 1e-6\n",
    "    g = group.copy()\n",
    "\n",
    "    # -- Spektrale Indizes ------------------------------------------------\n",
    "    g[\"NDVI\"] = (g.near_infrared - g.red) / (g.near_infrared + g.red + eps)\n",
    "    g[\"NBR\"]  = (g.near_infrared - g.shortwave_infrared_2) / (g.near_infrared + g.shortwave_infrared_2 + eps)\n",
    "    g[\"NDMI\"] = (g.near_infrared - g.shortwave_infrared_1) / (g.near_infrared + g.shortwave_infrared_1 + eps)\n",
    "    g[\"NDWI\"] = (g.near_infrared - g.shortwave_infrared_1) / (g.near_infrared + g.shortwave_infrared_1 + eps)\n",
    "\n",
    "    g[\"EVI\"]  = 2.5 * (g.near_infrared - g.red) / (g.near_infrared + 6 * g.red - 7.5 * g.blue + 1 + eps)\n",
    "    g[\"NBR2\"] = (g.shortwave_infrared_1 - g.shortwave_infrared_2) / (g.shortwave_infrared_1 + g.shortwave_infrared_2 + eps)\n",
    "\n",
    "    # -- Zeitliche Ableitungen (Differenzen & Lags) -----------------------\n",
    "    g[\"dNDVI\"] = g[\"NDVI\"].diff()\n",
    "    g[\"dNBR\"]  = g[\"NBR\"].shift(1) - g[\"NBR\"]          # Vorjahr−Aktuell\n",
    "\n",
    "    for lag in range(1, lags + 1):\n",
    "        g[f\"NDVI_lag{lag}\"]  = g[\"NDVI\"].shift(lag)\n",
    "        g[f\"dNDVI_lag{lag}\"] = g[\"dNDVI\"].shift(lag)\n",
    "        g[f\"NBR_lag{lag}\"]   = g[\"NBR\"].shift(lag)\n",
    "        g[f\"dNBR_lag{lag}\"]  = g[\"dNBR\"].shift(lag)\n",
    "\n",
    "    # -- Drei‑Jahres‑Statistik + Anomalien --------------------------------\n",
    "    g[\"var_NBR_3yr\"]   = g[\"NBR\"].shift(1).rolling(3, min_periods=3).var()\n",
    "    g[\"trend_NBR_3yr\"] = g[\"NBR\"] - g[\"NBR\"].shift(3)\n",
    "    g[\"z_NBR\"]         = (\n",
    "        (g[\"NBR\"] - g[\"NBR\"].shift(1).rolling(3, min_periods=2).mean()) /\n",
    "        (g[\"NBR\"].shift(1).rolling(3, min_periods=2).std() + eps)\n",
    "    )\n",
    "\n",
    "    # -- Disturbance‑Proxies ---------------------------------------------\n",
    "    g[\"d2NBR\"] = g[\"dNBR\"].diff()\n",
    "\n",
    "    return g\n",
    "\n",
    "# Feature‑Engineering auf beiden Splits anwenden\n",
    "train_feat = (\n",
    "    train_df.groupby(\"forest_id\", group_keys=False)\n",
    "            .apply(engineer_features)\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    ")\n",
    "val_feat = (\n",
    "    val_df.groupby(\"forest_id\", group_keys=False)\n",
    "          .apply(engineer_features)\n",
    "          .dropna()\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# X / y‑Matrizen\n",
    "EXCLUDE = [\"is_disturbance\", \"fid\", \"forest_id\", \"year\", \"row\", \"col\"]\n",
    "FEATURES = [c for c in train_feat.columns if c not in EXCLUDE]\n",
    "\n",
    "X_train = train_feat[FEATURES].values\n",
    "y_train = train_feat[\"is_disturbance\"].values\n",
    "X_val   = val_feat[FEATURES].values\n",
    "y_val   = val_feat[\"is_disturbance\"].values\n",
    "years_train = train_feat[\"year\"].values          # für GroupKFold\n",
    "\n",
    "print(f\"🛈  {len(FEATURES)} Feature‑Spalten erzeugt.\")\n"
   ],
   "id": "6680c708c5cf4a0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════════════\n",
      "FEATURE ENGINEERING\n",
      "══════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/s6j5yq4s12j6wvms4084h50h0000gn/T/ipykernel_64479/1540098449.py:46: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(engineer_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛈  30 Feature‑Spalten erzeugt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/s6j5yq4s12j6wvms4084h50h0000gn/T/ipykernel_64479/1540098449.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(engineer_features)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:49:43.686135Z",
     "start_time": "2025-05-09T12:49:32.347252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# 4 │ 5‑fach GroupKFold‑CV  (Gruppen = Jahr)                             │\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "banner(\"5‑FOLD CV – LIVE‑METRIKEN\")\n",
    "\n",
    "N_SPLITS = 5\n",
    "MIN_RECALL = 0.6\n",
    "BETA = 2                                 # für F‑Beta\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "# Basis‑Hyperparameter für XGBoost\n",
    "xgb_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",                 # PR‑AUC\n",
    "    eta=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=2,\n",
    "    gamma=0.5,                           # Minimale Verlustsreduktion für Split - Werte 0.1-1 reduzieren Overfitting\n",
    "    n_estimators=2200,\n",
    "    scale_pos_weight = np.bincount(y_train)[0] / np.bincount(y_train)[1],\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "# Optional: GridSearch für optimalen gamma-Wert\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Sample-Subset für schnellere Suche\n",
    "sample_size = min(10000, len(X_train))\n",
    "indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "X_sample, y_sample = X_train[indices], y_train[indices]\n",
    "\n",
    "# Basis-Modell für Grid Search\n",
    "search_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",\n",
    "    eta=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=2,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_estimators=200  # Reduziert für schnellere Suche\n",
    ")\n",
    "\n",
    "# Parameter-Grid\n",
    "param_grid = {\n",
    "    'gamma': [0, 0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "# Grid Search durchführen\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=search_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 3-fach CV für Geschwindigkeit\n",
    "    scoring='average_precision',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"GridSearch für optimalen gamma-Parameter...\")\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "best_gamma = grid_search.best_params_['gamma']\n",
    "print(f\"Bester gamma-Wert: {best_gamma}\")\n",
    "\n",
    "# Update gamma-Wert in Hauptmodell-Parametern\n",
    "xgb_params['gamma'] = best_gamma\n",
    "print(f\"Haupt-xgb_params aktualisiert mit gamma={best_gamma}\")\n",
    "\n",
    "thresholds, fold_stats = [], []\n",
    "\n",
    "for k, (idx_tr, idx_va) in enumerate(gkf.split(X_train, y_train, groups=years_train), start=1):\n",
    "    X_tr, y_tr = X_train[idx_tr], y_train[idx_tr]\n",
    "    X_va, y_va = X_train[idx_va], y_train[idx_va]\n",
    "\n",
    "    # ---- Modell + Isotone‑Kalibrierung ---------------------------------\n",
    "    booster = XGBClassifier(**xgb_params).fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "    iso_clf = CalibratedClassifierCV(booster, method=\"isotonic\", cv=\"prefit\").fit(X_va, y_va)\n",
    "\n",
    "    probas = iso_clf.predict_proba(X_va)[:, 1]\n",
    "    precision, recall, thresh = precision_recall_curve(y_va, probas)\n",
    "\n",
    "    # gleiche Länge: precision/recall = n+1, thresholds = n\n",
    "    precision, recall = precision[:-1], recall[:-1]\n",
    "\n",
    "    mask = recall >= MIN_RECALL\n",
    "    if mask.any():\n",
    "        tau = thresh[mask][precision[mask].argmax()]\n",
    "    else:                                 # Fallback: bestes F‑Beta\n",
    "        f2 = (1 + BETA**2) * precision * recall / (BETA**2 * precision + recall + 1e-9)\n",
    "        tau = thresh[f2.argmax()]\n",
    "        logging.warning(f\"Fold {k}: Recall < {MIN_RECALL:.2f} – F2‑Optimum verwendet.\")\n",
    "\n",
    "    thresholds.append(tau)\n",
    "\n",
    "    y_pred = (probas >= tau).astype(int)\n",
    "    stats = dict(\n",
    "        pr_auc = average_precision_score(y_va, probas),\n",
    "        precision = precision_score(y_va, y_pred, zero_division=0),\n",
    "        recall = recall_score(y_va, y_pred, zero_division=0),\n",
    "        f1 = f1_score(y_va, y_pred, zero_division=0),\n",
    "        f2 = fbeta_score(y_va, y_pred, beta=BETA, zero_division=0),\n",
    "        bal_acc = balanced_accuracy_score(y_va, y_pred),\n",
    "    )\n",
    "    fold_stats.append(stats)\n",
    "\n",
    "    print(f\"Fold {k}:  PR‑AUC {stats['pr_auc']:.3f}  |  \"\n",
    "          f\"P {stats['precision']:.3f}  R {stats['recall']:.3f}  \"\n",
    "          f\"F1 {stats['f1']:.3f}  F2 {stats['f2']:.3f}\")\n",
    "\n",
    "# ---- Aggregierte CV‑Ergebnisse ----------------------------------------\n",
    "tau_star = float(np.median(thresholds))\n",
    "avg = pd.DataFrame(fold_stats).mean()\n",
    "\n",
    "print(\"\\n──────── Median‑Threshold & Ø‑Metriken ────────\")\n",
    "print(f\"Median τ*      : {tau_star:.3f}\")\n",
    "print(f\"Ø PR‑AUC       : {avg.pr_auc:.3f}\")\n",
    "print(f\"Ø Precision    : {avg.precision:.3f}\")\n",
    "print(f\"Ø Recall       : {avg.recall:.3f}\")\n",
    "print(f\"Ø F1‑Score     : {avg.f1:.3f}\")\n",
    "print(f\"Ø F2‑Score     : {avg.f2:.3f}\")\n",
    "print(f\"Ø BalancedAcc  : {avg.bal_acc:.3f}\")\n"
   ],
   "id": "3f0290ccd90ec7ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════════════\n",
      "5‑FOLD CV – LIVE‑METRIKEN\n",
      "══════════════════════════════════════════════════════════════════════════════\n",
      "GridSearch für optimalen gamma-Parameter...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Bester gamma-Wert: 1.0\n",
      "Haupt-xgb_params aktualisiert mit gamma=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:  PR‑AUC 0.582  |  P 0.550  R 0.619  F1 0.583  F2 0.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2:  PR‑AUC 0.465  |  P 0.419  R 0.606  F1 0.496  F2 0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3:  PR‑AUC 0.590  |  P 0.524  R 0.647  F1 0.579  F2 0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4:  PR‑AUC 0.533  |  P 0.519  R 0.603  F1 0.558  F2 0.584\n",
      "Fold 5:  PR‑AUC 0.495  |  P 0.502  R 0.621  F1 0.556  F2 0.593\n",
      "\n",
      "──────── Median‑Threshold & Ø‑Metriken ────────\n",
      "Median τ*      : 0.279\n",
      "Ø PR‑AUC       : 0.533\n",
      "Ø Precision    : 0.503\n",
      "Ø Recall       : 0.619\n",
      "Ø F1‑Score     : 0.554\n",
      "Ø F2‑Score     : 0.591\n",
      "Ø BalancedAcc  : 0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:30:56.135454Z",
     "start_time": "2025-05-09T12:30:56.133089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# 5 │ (Optional) Modell‑Persistenz                                       │\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "banner(\"SAVING MODEL\")\n",
    "\n",
    "MODEL_DIR = Path(\"Tim/models\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# joblib.dump(final_iso, MODEL_DIR / \"xgb_full_calibrated.joblib\")\n",
    "# with open(MODEL_DIR / \"best_threshold.txt\", \"w\") as f:\n",
    "#     f.write(f\"{tau_star:.6f}\")\n",
    "# fi.to_csv(MODEL_DIR / \"feature_importance.csv\")\n",
    "\n",
    "print(\"✓ Modelldateien wurden (sofern aktiviert) gespeichert.\")\n"
   ],
   "id": "88c74bde33ce229",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════════════\n",
      "SAVING MODEL\n",
      "══════════════════════════════════════════════════════════════════════════════\n",
      "✓ Modelldateien wurden (sofern aktiviert) gespeichert.\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
