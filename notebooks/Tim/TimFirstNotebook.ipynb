{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:30:16.240327Z",
     "start_time": "2025-05-09T12:30:16.236355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "#  WaldschÃ¤denâ€‘Detection  |  XGBoost  +  Isotone  |  Jahrâ€‘stratifizierte CV\n",
    "#  ---------------------------------------------------------------------------\n",
    "#  - nutzt PRâ€‘AUC ('aucpr') als Optimierungskriterium\n",
    "#  - 5â€‘fach GroupKFoldâ€‘Crossâ€‘Validation (Gruppen = Jahr)\n",
    "#  - ermittelt Ï„* (F2â€‘Optimum bei Recall â‰¥ 0.60) je Fold, nimmt Median\n",
    "# =============================================================================\n"
   ],
   "id": "66eb19512d2a697c",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:30:16.283520Z",
     "start_time": "2025-05-09T12:30:16.279538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# 1 â”‚ Bibliotheken & Logging                                             â”‚\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import datetime\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (average_precision_score, balanced_accuracy_score,\n",
    "                             confusion_matrix, fbeta_score, f1_score,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Logging\n",
    "# -----------------------------------------------------------------------\n",
    "LOG_DIR = Path(\"Tim/logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "log_file = LOG_DIR / f\"run_{datetime.datetime.now():%Y%m%d_%H%M%S}.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "\n",
    "# Helper fÃ¼r optische Ãœberschriften in Notebookâ€‘Ausgabe\n",
    "def banner(text: str) -> None:\n",
    "    line = \"â•\" * 78\n",
    "    print(f\"\\n{line}\\n{text}\\n{line}\")\n",
    "\n",
    "# Reproduzierbarkeit\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "logging.info(\"Notebook gestartet â€“ Seeds gesetzt (42).\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:30:16.450508Z",
     "start_time": "2025-05-09T12:30:16.311492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# 2 â”‚ Daten laden  &  Leakageâ€‘PrÃ¼fung                                    â”‚\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "banner(\"DATA LOADING\")\n",
    "\n",
    "DATA_DIR = Path(\"data\")                       # <â€‘â€‘ Pfad ggf. anpassen\n",
    "train_df = (\n",
    "    pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "      .sort_values([\"forest_id\", \"year\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "val_df = (\n",
    "    pd.read_csv(DATA_DIR / \"validation.csv\")\n",
    "      .sort_values([\"forest_id\", \"year\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# forest_idâ€‘Leakage: gleiche FlÃ¤che darf nicht in beiden Splits sein\n",
    "# -----------------------------------------------------------------------\n",
    "overlap_ids = set(train_df.forest_id) & set(val_df.forest_id)\n",
    "if overlap_ids:\n",
    "    print(f\"âš ï¸  {len(overlap_ids)} forest_ids doppelt; werden aus Validation entfernt.\")\n",
    "    val_df = val_df[~val_df.forest_id.isin(overlap_ids)]\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Zeilenâ€‘/Spaltenâ€‘Koordinaten (142 Ã— 142 Raster) anlegen, falls nicht vorhanden\n",
    "# -----------------------------------------------------------------------\n",
    "for df in (train_df, val_df):\n",
    "    if {\"row\", \"col\"}.issubset(df.columns):\n",
    "        continue\n",
    "    n_rows = n_cols = 142\n",
    "    df[\"row\"] = df[\"forest_id\"] // n_cols\n",
    "    df[\"col\"] = df[\"forest_id\"] % n_cols\n"
   ],
   "id": "dfe1c757215853f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "DATA LOADING\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:30:44.492042Z",
     "start_time": "2025-05-09T12:30:16.475429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# 3 â”‚ Feature Engineering                                                â”‚\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "banner(\"FEATURE ENGINEERING\")\n",
    "\n",
    "def engineer_features(group: pd.DataFrame, lags: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Erzeugt spektrale Indizes + zeitliche Ableitungen fÃ¼r eine Waldâ€‘ID.\"\"\"\n",
    "    eps = 1e-6\n",
    "    g = group.copy()\n",
    "\n",
    "    # -- Spektrale Indizes ------------------------------------------------\n",
    "    g[\"NDVI\"] = (g.near_infrared - g.red) / (g.near_infrared + g.red + eps)\n",
    "    g[\"NBR\"]  = (g.near_infrared - g.shortwave_infrared_2) / (g.near_infrared + g.shortwave_infrared_2 + eps)\n",
    "    g[\"NDMI\"] = (g.near_infrared - g.shortwave_infrared_1) / (g.near_infrared + g.shortwave_infrared_1 + eps)\n",
    "    g[\"NDWI\"] = (g.near_infrared - g.shortwave_infrared_1) / (g.near_infrared + g.shortwave_infrared_1 + eps)\n",
    "\n",
    "    g[\"EVI\"]  = 2.5 * (g.near_infrared - g.red) / (g.near_infrared + 6 * g.red - 7.5 * g.blue + 1 + eps)\n",
    "    g[\"NBR2\"] = (g.shortwave_infrared_1 - g.shortwave_infrared_2) / (g.shortwave_infrared_1 + g.shortwave_infrared_2 + eps)\n",
    "\n",
    "    # -- Zeitliche Ableitungen (Differenzen & Lags) -----------------------\n",
    "    g[\"dNDVI\"] = g[\"NDVI\"].diff()\n",
    "    g[\"dNBR\"]  = g[\"NBR\"].shift(1) - g[\"NBR\"]          # Vorjahrâˆ’Aktuell\n",
    "\n",
    "    for lag in range(1, lags + 1):\n",
    "        g[f\"NDVI_lag{lag}\"]  = g[\"NDVI\"].shift(lag)\n",
    "        g[f\"dNDVI_lag{lag}\"] = g[\"dNDVI\"].shift(lag)\n",
    "        g[f\"NBR_lag{lag}\"]   = g[\"NBR\"].shift(lag)\n",
    "        g[f\"dNBR_lag{lag}\"]  = g[\"dNBR\"].shift(lag)\n",
    "\n",
    "    # -- Dreiâ€‘Jahresâ€‘Statistik + Anomalien --------------------------------\n",
    "    g[\"var_NBR_3yr\"]   = g[\"NBR\"].shift(1).rolling(3, min_periods=3).var()\n",
    "    g[\"trend_NBR_3yr\"] = g[\"NBR\"] - g[\"NBR\"].shift(3)\n",
    "    g[\"z_NBR\"]         = (\n",
    "        (g[\"NBR\"] - g[\"NBR\"].shift(1).rolling(3, min_periods=2).mean()) /\n",
    "        (g[\"NBR\"].shift(1).rolling(3, min_periods=2).std() + eps)\n",
    "    )\n",
    "\n",
    "    # -- Disturbanceâ€‘Proxies ---------------------------------------------\n",
    "    g[\"d2NBR\"] = g[\"dNBR\"].diff()\n",
    "\n",
    "    return g\n",
    "\n",
    "# Featureâ€‘Engineering auf beiden Splits anwenden\n",
    "train_feat = (\n",
    "    train_df.groupby(\"forest_id\", group_keys=False)\n",
    "            .apply(engineer_features)\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    ")\n",
    "val_feat = (\n",
    "    val_df.groupby(\"forest_id\", group_keys=False)\n",
    "          .apply(engineer_features)\n",
    "          .dropna()\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# X / yâ€‘Matrizen\n",
    "EXCLUDE = [\"is_disturbance\", \"fid\", \"forest_id\", \"year\", \"row\", \"col\"]\n",
    "FEATURES = [c for c in train_feat.columns if c not in EXCLUDE]\n",
    "\n",
    "X_train = train_feat[FEATURES].values\n",
    "y_train = train_feat[\"is_disturbance\"].values\n",
    "X_val   = val_feat[FEATURES].values\n",
    "y_val   = val_feat[\"is_disturbance\"].values\n",
    "years_train = train_feat[\"year\"].values          # fÃ¼r GroupKFold\n",
    "\n",
    "print(f\"ğŸ›ˆ  {len(FEATURES)} Featureâ€‘Spalten erzeugt.\")\n"
   ],
   "id": "6680c708c5cf4a0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "FEATURE ENGINEERING\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/s6j5yq4s12j6wvms4084h50h0000gn/T/ipykernel_64479/1540098449.py:46: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(engineer_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›ˆ  30 Featureâ€‘Spalten erzeugt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/s6j5yq4s12j6wvms4084h50h0000gn/T/ipykernel_64479/1540098449.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(engineer_features)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:49:43.686135Z",
     "start_time": "2025-05-09T12:49:32.347252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# 4 â”‚ 5â€‘fach GroupKFoldâ€‘CV  (Gruppen = Jahr)                             â”‚\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "banner(\"5â€‘FOLD CV â€“ LIVEâ€‘METRIKEN\")\n",
    "\n",
    "N_SPLITS = 5\n",
    "MIN_RECALL = 0.6\n",
    "BETA = 2                                 # fÃ¼r Fâ€‘Beta\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "# Basisâ€‘Hyperparameter fÃ¼r XGBoost\n",
    "xgb_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",                 # PRâ€‘AUC\n",
    "    eta=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=2,\n",
    "    gamma=0.5,                           # Minimale Verlustsreduktion fÃ¼r Split - Werte 0.1-1 reduzieren Overfitting\n",
    "    n_estimators=2200,\n",
    "    scale_pos_weight = np.bincount(y_train)[0] / np.bincount(y_train)[1],\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "# Optional: GridSearch fÃ¼r optimalen gamma-Wert\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Sample-Subset fÃ¼r schnellere Suche\n",
    "sample_size = min(10000, len(X_train))\n",
    "indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "X_sample, y_sample = X_train[indices], y_train[indices]\n",
    "\n",
    "# Basis-Modell fÃ¼r Grid Search\n",
    "search_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",\n",
    "    eta=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=2,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_estimators=200  # Reduziert fÃ¼r schnellere Suche\n",
    ")\n",
    "\n",
    "# Parameter-Grid\n",
    "param_grid = {\n",
    "    'gamma': [0, 0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "# Grid Search durchfÃ¼hren\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=search_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 3-fach CV fÃ¼r Geschwindigkeit\n",
    "    scoring='average_precision',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"GridSearch fÃ¼r optimalen gamma-Parameter...\")\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "best_gamma = grid_search.best_params_['gamma']\n",
    "print(f\"Bester gamma-Wert: {best_gamma}\")\n",
    "\n",
    "# Update gamma-Wert in Hauptmodell-Parametern\n",
    "xgb_params['gamma'] = best_gamma\n",
    "print(f\"Haupt-xgb_params aktualisiert mit gamma={best_gamma}\")\n",
    "\n",
    "thresholds, fold_stats = [], []\n",
    "\n",
    "for k, (idx_tr, idx_va) in enumerate(gkf.split(X_train, y_train, groups=years_train), start=1):\n",
    "    X_tr, y_tr = X_train[idx_tr], y_train[idx_tr]\n",
    "    X_va, y_va = X_train[idx_va], y_train[idx_va]\n",
    "\n",
    "    # ---- Modell + Isotoneâ€‘Kalibrierung ---------------------------------\n",
    "    booster = XGBClassifier(**xgb_params).fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "    iso_clf = CalibratedClassifierCV(booster, method=\"isotonic\", cv=\"prefit\").fit(X_va, y_va)\n",
    "\n",
    "    probas = iso_clf.predict_proba(X_va)[:, 1]\n",
    "    precision, recall, thresh = precision_recall_curve(y_va, probas)\n",
    "\n",
    "    # gleiche LÃ¤nge: precision/recall = n+1, thresholds = n\n",
    "    precision, recall = precision[:-1], recall[:-1]\n",
    "\n",
    "    mask = recall >= MIN_RECALL\n",
    "    if mask.any():\n",
    "        tau = thresh[mask][precision[mask].argmax()]\n",
    "    else:                                 # Fallback: bestes Fâ€‘Beta\n",
    "        f2 = (1 + BETA**2) * precision * recall / (BETA**2 * precision + recall + 1e-9)\n",
    "        tau = thresh[f2.argmax()]\n",
    "        logging.warning(f\"Fold {k}: Recall < {MIN_RECALL:.2f} â€“ F2â€‘Optimum verwendet.\")\n",
    "\n",
    "    thresholds.append(tau)\n",
    "\n",
    "    y_pred = (probas >= tau).astype(int)\n",
    "    stats = dict(\n",
    "        pr_auc = average_precision_score(y_va, probas),\n",
    "        precision = precision_score(y_va, y_pred, zero_division=0),\n",
    "        recall = recall_score(y_va, y_pred, zero_division=0),\n",
    "        f1 = f1_score(y_va, y_pred, zero_division=0),\n",
    "        f2 = fbeta_score(y_va, y_pred, beta=BETA, zero_division=0),\n",
    "        bal_acc = balanced_accuracy_score(y_va, y_pred),\n",
    "    )\n",
    "    fold_stats.append(stats)\n",
    "\n",
    "    print(f\"Fold {k}:  PRâ€‘AUC {stats['pr_auc']:.3f}  |  \"\n",
    "          f\"P {stats['precision']:.3f}  R {stats['recall']:.3f}  \"\n",
    "          f\"F1 {stats['f1']:.3f}  F2 {stats['f2']:.3f}\")\n",
    "\n",
    "# ---- Aggregierte CVâ€‘Ergebnisse ----------------------------------------\n",
    "tau_star = float(np.median(thresholds))\n",
    "avg = pd.DataFrame(fold_stats).mean()\n",
    "\n",
    "print(\"\\nâ”€â”€â”€â”€â”€â”€â”€â”€ Medianâ€‘Threshold & Ã˜â€‘Metriken â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"Median Ï„*      : {tau_star:.3f}\")\n",
    "print(f\"Ã˜ PRâ€‘AUC       : {avg.pr_auc:.3f}\")\n",
    "print(f\"Ã˜ Precision    : {avg.precision:.3f}\")\n",
    "print(f\"Ã˜ Recall       : {avg.recall:.3f}\")\n",
    "print(f\"Ã˜ F1â€‘Score     : {avg.f1:.3f}\")\n",
    "print(f\"Ã˜ F2â€‘Score     : {avg.f2:.3f}\")\n",
    "print(f\"Ã˜ BalancedAcc  : {avg.bal_acc:.3f}\")\n"
   ],
   "id": "3f0290ccd90ec7ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "5â€‘FOLD CV â€“ LIVEâ€‘METRIKEN\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "GridSearch fÃ¼r optimalen gamma-Parameter...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Bester gamma-Wert: 1.0\n",
      "Haupt-xgb_params aktualisiert mit gamma=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:  PRâ€‘AUC 0.582  |  P 0.550  R 0.619  F1 0.583  F2 0.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2:  PRâ€‘AUC 0.465  |  P 0.419  R 0.606  F1 0.496  F2 0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3:  PRâ€‘AUC 0.590  |  P 0.524  R 0.647  F1 0.579  F2 0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4:  PRâ€‘AUC 0.533  |  P 0.519  R 0.603  F1 0.558  F2 0.584\n",
      "Fold 5:  PRâ€‘AUC 0.495  |  P 0.502  R 0.621  F1 0.556  F2 0.593\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€ Medianâ€‘Threshold & Ã˜â€‘Metriken â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Median Ï„*      : 0.279\n",
      "Ã˜ PRâ€‘AUC       : 0.533\n",
      "Ã˜ Precision    : 0.503\n",
      "Ã˜ Recall       : 0.619\n",
      "Ã˜ F1â€‘Score     : 0.554\n",
      "Ã˜ F2â€‘Score     : 0.591\n",
      "Ã˜ BalancedAcc  : 0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:30:56.135454Z",
     "start_time": "2025-05-09T12:30:56.133089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# 5 â”‚ (Optional) Modellâ€‘Persistenz                                       â”‚\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "banner(\"SAVING MODEL\")\n",
    "\n",
    "MODEL_DIR = Path(\"Tim/models\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# joblib.dump(final_iso, MODEL_DIR / \"xgb_full_calibrated.joblib\")\n",
    "# with open(MODEL_DIR / \"best_threshold.txt\", \"w\") as f:\n",
    "#     f.write(f\"{tau_star:.6f}\")\n",
    "# fi.to_csv(MODEL_DIR / \"feature_importance.csv\")\n",
    "\n",
    "print(\"âœ“ Modelldateien wurden (sofern aktiviert) gespeichert.\")\n"
   ],
   "id": "88c74bde33ce229",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "SAVING MODEL\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "âœ“ Modelldateien wurden (sofern aktiviert) gespeichert.\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
