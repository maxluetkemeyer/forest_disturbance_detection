#%%
# =============================================================================
#  WaldschÃ¤denâ€‘Detection  |  XGBoostâ€¯+â€¯Isotone  |  Jahrâ€‘stratifizierte CV
#  ---------------------------------------------------------------------------
#  - verhindert forest_idâ€‘Leakage zwischen Train / Validation
#  - korrigiert NDWIâ€‘Formel (Gaoâ€‘Variante)
#  - nutzt PRâ€‘AUC ('aucpr') als Optimierungskriterium
#  - 5â€‘fach GroupKFoldâ€‘Crossâ€‘Validation (Gruppen = Jahr)
#  - ermittelt Ï„* (F2â€‘Optimum bei Recall â‰¥ 0.60) je Fold, nimmt Median
#  - **Logik & Ergebnisse bleiben unverÃ¤ndert â€“ Code nur lesbarer!**
# =============================================================================

#%%
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# 1 â”‚ Bibliotheken & Logging                                             â”‚
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import datetime
import logging
import random
from pathlib import Path

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import (average_precision_score, balanced_accuracy_score,
                             confusion_matrix, fbeta_score, f1_score,
                             precision_recall_curve, precision_score,
                             recall_score)
from sklearn.model_selection import GroupKFold
from xgboost import XGBClassifier

# -----------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------
LOG_DIR = Path("Tim/logs")
LOG_DIR.mkdir(parents=True, exist_ok=True)
log_file = LOG_DIR / f"run_{datetime.datetime.now():%Y%m%d_%H%M%S}.log"

logging.basicConfig(
    filename=log_file,
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)

# Helper fÃ¼r optische Ãœberschriften in Notebookâ€‘Ausgabe
def banner(text: str) -> None:
    line = "â•" * 78
    print(f"\n{line}\n{text}\n{line}")

# Reproduzierbarkeit
np.random.seed(42)
random.seed(42)
logging.info("Notebook gestartet â€“ Seeds gesetzt (42).")

#%%
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# 2 â”‚ Daten laden  &  Leakageâ€‘PrÃ¼fung                                    â”‚
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
banner("DATA LOADING")

DATA_DIR = Path("data")                       # <â€‘â€‘ Pfad ggf. anpassen
train_df = (
    pd.read_csv(DATA_DIR / "train.csv")
      .sort_values(["forest_id", "year"])
      .reset_index(drop=True)
)
val_df = (
    pd.read_csv(DATA_DIR / "validation.csv")
      .sort_values(["forest_id", "year"])
      .reset_index(drop=True)
)

# -----------------------------------------------------------------------
# forest_idâ€‘Leakage: gleiche FlÃ¤che darf nicht in beiden Splits sein
# -----------------------------------------------------------------------
overlap_ids = set(train_df.forest_id) & set(val_df.forest_id)
if overlap_ids:
    print(f"âš ï¸  {len(overlap_ids)} forest_ids doppelt; werden aus Validation entfernt.")
    val_df = val_df[~val_df.forest_id.isin(overlap_ids)]

# -----------------------------------------------------------------------
# Zeilenâ€‘/Spaltenâ€‘Koordinaten (142 Ã— 142 Raster) anlegen, falls nicht vorhanden
# -----------------------------------------------------------------------
for df in (train_df, val_df):
    if {"row", "col"}.issubset(df.columns):
        continue
    n_rows = n_cols = 142
    df["row"] = df["forest_id"] // n_cols
    df["col"] = df["forest_id"] % n_cols

#%%
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# 3 â”‚ Feature Engineering                                                â”‚
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
banner("FEATURE ENGINEERING")

def engineer_features(group: pd.DataFrame, lags: int = 3) -> pd.DataFrame:
    """Erzeugt spektrale Indizes + zeitliche Ableitungen fÃ¼r eine Waldâ€‘ID."""
    eps = 1e-6
    g = group.copy()

    # -- Spektrale Indizes ------------------------------------------------
    g["NDVI"] = (g.near_infrared - g.red) / (g.near_infrared + g.red + eps)
    g["NBR"]  = (g.near_infrared - g.shortwave_infrared_2) / (g.near_infrared + g.shortwave_infrared_2 + eps)
    g["NDMI"] = (g.near_infrared - g.shortwave_infrared_1) / (g.near_infrared + g.shortwave_infrared_1 + eps)
    g["NDWI"] = (g.near_infrared - g.shortwave_infrared_1) / (g.near_infrared + g.shortwave_infrared_1 + eps)

    g["EVI"]  = 2.5 * (g.near_infrared - g.red) / (g.near_infrared + 6 * g.red - 7.5 * g.blue + 1 + eps)
    g["NBR2"] = (g.shortwave_infrared_1 - g.shortwave_infrared_2) / (g.shortwave_infrared_1 + g.shortwave_infrared_2 + eps)

    # -- Zeitliche Ableitungen (Differenzen & Lags) -----------------------
    g["dNDVI"] = g["NDVI"].diff()
    g["dNBR"]  = g["NBR"].shift(1) - g["NBR"]          # Vorjahrâˆ’Aktuell

    for lag in range(1, lags + 1):
        g[f"NDVI_lag{lag}"]  = g["NDVI"].shift(lag)
        g[f"dNDVI_lag{lag}"] = g["dNDVI"].shift(lag)
        g[f"NBR_lag{lag}"]   = g["NBR"].shift(lag)
        g[f"dNBR_lag{lag}"]  = g["dNBR"].shift(lag)

    # -- Dreiâ€‘Jahresâ€‘Statistik + Anomalien --------------------------------
    g["var_NBR_3yr"]   = g["NBR"].shift(1).rolling(3, min_periods=3).var()
    g["trend_NBR_3yr"] = g["NBR"] - g["NBR"].shift(3)
    g["z_NBR"]         = (
        (g["NBR"] - g["NBR"].shift(1).rolling(3, min_periods=2).mean()) /
        (g["NBR"].shift(1).rolling(3, min_periods=2).std() + eps)
    )

    # -- Disturbanceâ€‘Proxies ---------------------------------------------
    g["d2NBR"] = g["dNBR"].diff()

    return g

# Featureâ€‘Engineering auf beiden Splits anwenden
train_feat = (
    train_df.groupby("forest_id", group_keys=False)
            .apply(engineer_features)
            .dropna()
            .reset_index(drop=True)
)
val_feat = (
    val_df.groupby("forest_id", group_keys=False)
          .apply(engineer_features)
          .dropna()
          .reset_index(drop=True)
)

# X / yâ€‘Matrizen
EXCLUDE = ["is_disturbance", "fid", "forest_id", "year", "row", "col"]
FEATURES = [c for c in train_feat.columns if c not in EXCLUDE]

X_train = train_feat[FEATURES].values
y_train = train_feat["is_disturbance"].values
X_val   = val_feat[FEATURES].values
y_val   = val_feat["is_disturbance"].values
years_train = train_feat["year"].values          # fÃ¼r GroupKFold

print(f"ğŸ›ˆ  {len(FEATURES)} Featureâ€‘Spalten erzeugt.")

#%%
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# 4 â”‚ 5â€‘fach GroupKFoldâ€‘CV  (Gruppen = Jahr)                             â”‚
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
banner("5â€‘FOLD CV â€“ LIVEâ€‘METRIKEN")

N_SPLITS = 5
MIN_RECALL = 0.60
BETA = 2                                 # fÃ¼r Fâ€‘Beta

gkf = GroupKFold(n_splits=N_SPLITS)

# Basisâ€‘Hyperparameter fÃ¼r XGBoost
xgb_params = dict(
    objective="binary:logistic",
    eval_metric="aucpr",                 # PRâ€‘AUC
    eta=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=2,
    gamma=0.5,                           # Minimale Verlustsreduktion fÃ¼r Split - Werte 0.1-1 reduzieren Overfitting
    n_estimators=2200,
    scale_pos_weight = np.bincount(y_train)[0] / np.bincount(y_train)[1],
    tree_method="hist",
    random_state=42,
    n_jobs=-1,
    early_stopping_rounds=50,
)

# Optional: GridSearch fÃ¼r optimalen gamma-Wert
from sklearn.model_selection import GridSearchCV

# Sample-Subset fÃ¼r schnellere Suche
sample_size = min(10000, len(X_train))
indices = np.random.choice(len(X_train), sample_size, replace=False)
X_sample, y_sample = X_train[indices], y_train[indices]

# Basis-Modell fÃ¼r Grid Search
search_model = XGBClassifier(
    objective="binary:logistic",
    eval_metric="aucpr",
    eta=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=2,
    tree_method="hist",
    random_state=42,
    n_estimators=200  # Reduziert fÃ¼r schnellere Suche
)

# Parameter-Grid
param_grid = {
    'gamma': [0, 0.1, 0.3, 0.5, 0.7, 1.0]
}

# Grid Search durchfÃ¼hren
grid_search = GridSearchCV(
    estimator=search_model,
    param_grid=param_grid,
    cv=3,  # 3-fach CV fÃ¼r Geschwindigkeit
    scoring='average_precision',
    verbose=1
)

print("GridSearch fÃ¼r optimalen gamma-Parameter...")
grid_search.fit(X_sample, y_sample)
best_gamma = grid_search.best_params_['gamma']
print(f"Bester gamma-Wert: {best_gamma}")

# Update gamma-Wert in Hauptmodell-Parametern
xgb_params['gamma'] = best_gamma
print(f"Haupt-xgb_params aktualisiert mit gamma={best_gamma}")

thresholds, fold_stats = [], []

for k, (idx_tr, idx_va) in enumerate(gkf.split(X_train, y_train, groups=years_train), start=1):
    X_tr, y_tr = X_train[idx_tr], y_train[idx_tr]
    X_va, y_va = X_train[idx_va], y_train[idx_va]

    # ---- Modell + Isotoneâ€‘Kalibrierung ---------------------------------
    booster = XGBClassifier(**xgb_params).fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)
    iso_clf = CalibratedClassifierCV(booster, method="isotonic", cv="prefit").fit(X_va, y_va)

    probas = iso_clf.predict_proba(X_va)[:, 1]
    precision, recall, thresh = precision_recall_curve(y_va, probas)

    # gleiche LÃ¤nge: precision/recall = n+1, thresholds = n
    precision, recall = precision[:-1], recall[:-1]

    mask = recall >= MIN_RECALL
    if mask.any():
        tau = thresh[mask][precision[mask].argmax()]
    else:                                 # Fallback: bestes Fâ€‘Beta
        f2 = (1 + BETA**2) * precision * recall / (BETA**2 * precision + recall + 1e-9)
        tau = thresh[f2.argmax()]
        logging.warning(f"Fold {k}: Recall < {MIN_RECALL:.2f} â€“ F2â€‘Optimum verwendet.")

    thresholds.append(tau)

    y_pred = (probas >= tau).astype(int)
    stats = dict(
        pr_auc = average_precision_score(y_va, probas),
        precision = precision_score(y_va, y_pred, zero_division=0),
        recall = recall_score(y_va, y_pred, zero_division=0),
        f1 = f1_score(y_va, y_pred, zero_division=0),
        f2 = fbeta_score(y_va, y_pred, beta=BETA, zero_division=0),
        bal_acc = balanced_accuracy_score(y_va, y_pred),
    )
    fold_stats.append(stats)

    print(f"Fold {k}:  PRâ€‘AUC {stats['pr_auc']:.3f}  |  "
          f"P {stats['precision']:.3f}  R {stats['recall']:.3f}  "
          f"F1 {stats['f1']:.3f}  F2 {stats['f2']:.3f}")

# ---- Aggregierte CVâ€‘Ergebnisse ----------------------------------------
tau_star = float(np.median(thresholds))
avg = pd.DataFrame(fold_stats).mean()

print("\nâ”€â”€â”€â”€â”€â”€â”€â”€ Medianâ€‘Threshold & Ã˜â€‘Metriken â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"Median Ï„*      : {tau_star:.3f}")
print(f"Ã˜ PRâ€‘AUC       : {avg.pr_auc:.3f}")
print(f"Ã˜ Precision    : {avg.precision:.3f}")
print(f"Ã˜ Recall       : {avg.recall:.3f}")
print(f"Ã˜ F1â€‘Score     : {avg.f1:.3f}")
print(f"Ã˜ F2â€‘Score     : {avg.f2:.3f}")
print(f"Ã˜ BalancedAcc  : {avg.bal_acc:.3f}")

#%%
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# 5 â”‚ (Optional) Modellâ€‘Persistenz                                       â”‚
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
banner("SAVING MODEL")

MODEL_DIR = Path("Tim/models")
MODEL_DIR.mkdir(parents=True, exist_ok=True)

# joblib.dump(final_iso, MODEL_DIR / "xgb_full_calibrated.joblib")
# with open(MODEL_DIR / "best_threshold.txt", "w") as f:
#     f.write(f"{tau_star:.6f}")
# fi.to_csv(MODEL_DIR / "feature_importance.csv")

print("âœ“ Modelldateien wurden (sofern aktiviert) gespeichert.")
