{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146db275",
   "metadata": {},
   "source": [
    "## Import numpy and matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56340c3e",
   "metadata": {},
   "source": [
    "## Linear Regression with Gradient Descent for a given loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e896ac",
   "metadata": {},
   "source": [
    "Next, let us create a class of Linear Regression that performs gradient descent for a given loss function. \n",
    "\n",
    "Exercise: **Please check the implementation and if it fits what we discussed about in class** --> Ask questions if this is not the case! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGradientDescent():\n",
    "    \"\"\"\n",
    "    Linear regression implementation (gradient descent)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss, eta=0.1, n_steps=5):\n",
    "        \"\"\"Initializes the linear regression model.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss function object that computes the loss and its gradient.\n",
    "            eta (float, optional): Learning rate. Defaults to 0.1.\n",
    "            n_steps (int, optional): Number of steps. Defaults to 5.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.n_steps = n_steps\n",
    "        self.loss = loss\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the linear regression model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "        y : Array of shape [n_samples, 1]\n",
    "        \"\"\"        \n",
    "\n",
    "        # make sure that we have multidimensional np arrays\n",
    "        X = np.array(X).reshape((X.shape[0], -1))\n",
    "        # IMPORTANT: Make sure that we have a column vector! \n",
    "        y = np.array(y).reshape((len(y), 1))\n",
    "        \n",
    "        # starting point\n",
    "        w = np.zeros((X.shape[1], 1))\n",
    "        \n",
    "        # gradient descent steps\n",
    "        for i in range(self.n_steps):\n",
    "\n",
    "            grad = self.loss.gradient(X, y, w)\n",
    "            w = w - self.eta * grad\n",
    "        self._w = w\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Computes predictions for a new set of points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : Array of shape [n_samples, 1]\n",
    "        \"\"\"                     \n",
    "\n",
    "        # make sure that we have multidimensional np arrays\n",
    "        X = np.array(X).reshape((X.shape[0], -1))        \n",
    "\n",
    "        # compute predictions\n",
    "        predictions = np.dot(X, self._w)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9305472",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ab442",
   "metadata": {},
   "source": [
    "Next, we define the loss functions from the slides. \n",
    "\n",
    "Exercise: **Please verify that the forward-methods of all classes really calculate the respective loss functions**\n",
    "\n",
    "You do not have to directly understand, why the gradient method works as given here. But you should know, why we need a gradient method in all loss classes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97470a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE loss\n",
    "\n",
    "class MSELoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, target, predictions):\n",
    "        \"\"\"\n",
    "        Computes the mean squared error loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : Array of shape [n_samples, 1]\n",
    "        predictions : Array of shape [n_samples, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "        \"\"\"                     \n",
    "\n",
    "        loss = np.mean((target - predictions) ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the mean squared error loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "        y : Array of shape [n_samples, 1]\n",
    "        w : Array of shape [n_features, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : Array of shape [n_features, 1]\n",
    "        \"\"\"                     \n",
    "\n",
    "        n = X.shape[0]\n",
    "        gradient = 2/n * np.dot(X.T, (np.dot(X, w) - y))\n",
    "\n",
    "        return gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c81c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAE loss\n",
    "\n",
    "class MAELoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, target, predictions):\n",
    "        \"\"\"\n",
    "        Computes the mean absolute error loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : Array of shape [n_samples, 1]\n",
    "        predictions : Array of shape [n_samples, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "        \"\"\"                     \n",
    "\n",
    "        loss = np.mean(np.abs(target - predictions))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the mean absolute error loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "        y : Array of shape [n_samples, 1]\n",
    "        w : Array of shape [n_features, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : Array of shape [n_features, 1]\n",
    "        \"\"\"                     \n",
    "\n",
    "        n = X.shape[0]\n",
    "        gradient = 1/n * np.dot(X.T, np.sign(np.dot(X, w) - y))\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Huber loss\n",
    "\n",
    "class HuberLoss():\n",
    "    def __init__(self, delta=1):\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, target, predictions):\n",
    "        \"\"\"\n",
    "        Computes the Huber loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : Array of shape [n_samples, 1]\n",
    "        predictions : Array of shape [n_samples, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "        \"\"\"                     \n",
    "\n",
    "        loss = np.mean(np.where(np.abs(target - predictions) < self.delta, 0.5 * (target - predictions) ** 2, self.delta * np.abs(target - predictions) - 0.5 * self.delta ** 2))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the Huber loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "        y : Array of shape [n_samples, 1]\n",
    "        w : Array of shape [n_features, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : Array of shape [n_features, 1]\n",
    "        \"\"\"                     \n",
    "\n",
    "        n = X.shape[0]\n",
    "        gradient = np.dot(X.T, np.where(np.abs(np.dot(X, w) - y) < self.delta, np.dot(X, w) - y, self.delta * np.sign(np.dot(X, w) - y)))\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed237f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## epsilon-insensitive loss\n",
    "\n",
    "class EpsilonInsensitiveLoss():\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, target, predictions):\n",
    "        \"\"\"\n",
    "        Computes the epsilon-insensitive loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : Array of shape [n_samples, 1]\n",
    "        predictions : Array of shape [n_samples, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "        \"\"\"                     \n",
    "\n",
    "        loss = np.mean(np.maximum(0, np.abs(target - predictions) - self.epsilon))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the epsilon-insensitive loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "        y : Array of shape [n_samples, 1]\n",
    "        w : Array of shape [n_features, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : Array of shape [n_features, 1]\n",
    "        \"\"\"                     \n",
    "\n",
    "        n = X.shape[0]\n",
    "        gradient = np.dot(X.T, np.where(np.abs(np.dot(X, w) - y) > self.epsilon, np.sign(np.dot(X, w) - y), 0))\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pinball loss\n",
    "\n",
    "class PinballLoss():\n",
    "    def __init__(self, tau=0.5):\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, target, predictions):\n",
    "        \"\"\"\n",
    "        Computes the pinball loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : Array of shape [n_samples, 1]\n",
    "        predictions : Array of shape [n_samples, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "        \"\"\"                     \n",
    "\n",
    "        loss = np.mean(np.where(target <= predictions, (1-self.tau) * (predictions - target), self.tau * (target - predictions)))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the pinball loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "        y : Array of shape [n_samples, 1]\n",
    "        w : Array of shape [n_features, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : Array of shape [n_features, 1]\n",
    "        \"\"\"                     \n",
    "\n",
    "        n = X.shape[0]\n",
    "        gradient = np.dot(X.T, np.where(np.dot(X, w) >= y, 1-self.tau, - self.tau))\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a5136d",
   "metadata": {},
   "source": [
    "## Train a constant with different losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "y = np.random.exponential(scale=2.0, size=1000)\n",
    "\n",
    "## For testing purposes at home, you can also use other datasets:\n",
    "#y = np.random.lognormal(size= 1000, mean=0, sigma=0.5)\n",
    "# y = np.random.chisquare(df=2, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7620f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(y, bins=50, color='blue', alpha=0.5)\n",
    "plt.grid()\n",
    "plt.title('Histogram of y')\n",
    "plt.xticks(np.arange(0, 20, 0.5), rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c771ea4",
   "metadata": {},
   "source": [
    "Exercise: **Try to fit a constant without computer**:\n",
    "What would you guess, which constant will be fitted using \n",
    "- mean squared error\n",
    "- mean absolute error\n",
    "- Huber-loss with $\\delta = 1$\n",
    "- $\\epsilon$-insensitive loss with $\\epsilon = 0.1$\n",
    "- $\\epsilon$-insensitive loss with $\\epsilon = 1$\n",
    "- pinball-loss with $\\tau = 0.25$\n",
    "- pinball-loss with $\\tau = 0.75$\n",
    "Write down your answers in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c0cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_guess_mse =  ## Guess for MSE loss\n",
    "w_guess_mae =  ## Guess for MAE loss\n",
    "w_guess_huber =  ## Guess for Huber loss with delta=1\n",
    "w_guess_epsilon01 =  ## Guess for epsilon-insensitive loss with epsilon=0.1\n",
    "w_guess_epsilon1 =  ## Guess for epsilon-insensitive loss with epsilon=1\n",
    "w_guess_pinball025 =  ## Guess for pinball loss with tau=0.25\n",
    "w_guess_pinball075 =  ## Guess for pinball loss with tau=0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2dbed",
   "metadata": {},
   "source": [
    "Exercise: **Complete the method fit_constant(loss_fn, y) below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59da788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_constant(loss_fn, y):\n",
    "    \"\"\"\n",
    "    Fits a constant model using the given loss function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_fn : Loss function object\n",
    "    y : Array of shape [n_samples, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    # create a constant model\n",
    "    X = \n",
    "\n",
    "    # fit the model using the loss function\n",
    "    eta = 0.001\n",
    "    n_steps = 10000\n",
    "    model = \n",
    "    model.fit(...)\n",
    "\n",
    "    return model._w[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad84ecc",
   "metadata": {},
   "source": [
    "Now we can perform gradient descent in order to fit all constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_names = ['MSE', 'MAE', 'Huber', 'EpsilonInsensitive (epsilon=0.1)', 'EpsilonInsensitive (epsilon=1)', 'Pinball (tau=0.25)', 'Pinball (tau=0.75)']\n",
    "loss_functions = [MSELoss(), MAELoss(), HuberLoss(delta=1), EpsilonInsensitiveLoss(epsilon=0.1), EpsilonInsensitiveLoss(epsilon=1), PinballLoss(tau=0.25), PinballLoss(tau=0.75)]\n",
    "loss_guesses = [w_guess_mse, w_guess_mae, w_guess_huber, w_guess_epsilon01, w_guess_epsilon1, w_guess_pinball025, w_guess_pinball075] \n",
    "constant_pred = []\n",
    "for f in range(len(loss_functions)):\n",
    "    loss_fn = loss_functions[f]\n",
    "    loss_name = loss_names[f]\n",
    "    loss_guess = loss_guesses[f]\n",
    "    w = fit_constant(loss_fn, y)\n",
    "    constant_pred.append(w)\n",
    "    print(f\"Constant model using {loss_name}: {w:.2f}\")\n",
    "    print(f\"Guess for {loss_name}: {loss_guess:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d70b3",
   "metadata": {},
   "source": [
    "Let us also plot the fitted constants. Is it in line with your expectation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'orange', 'green', 'red', 'black', 'pink', 'purple']\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(y, bins=50, color='blue', alpha=0.5)\n",
    "for f in range(len(loss_functions)):\n",
    "    loss_name = loss_names[f]\n",
    "    plt.axvline(x=constant_pred[f], color = colors[f], linestyle='--', label=f'{loss_name} prediction: {constant_pred[f]:.2f}')\n",
    "plt.title('Constant model predictions using different loss functions')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd32fc2",
   "metadata": {},
   "source": [
    "## Create Dataset with outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7e1d7",
   "metadata": {},
   "source": [
    "Now we create a dataset with an outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cebd5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some artificial data to be fitted\n",
    "m = 20\n",
    "w0_true = 4\n",
    "w1_true = 2\n",
    "w = np.array([w0_true, w1_true])\n",
    "\n",
    "X = np.linspace(-1,1,m).reshape(m, 1)\n",
    "ones = np.ones((X.shape[0], 1))\n",
    "X = np.concatenate((ones, X), axis=1)\n",
    "\n",
    "y = np.dot(X, w) + np.random.randn(m) * 0.4\n",
    "\n",
    "y[0] += 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:,-1], y, color='blue')\n",
    "plt.title('Data with outlier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67559f0e",
   "metadata": {},
   "source": [
    "## Try out different loss functions for our dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d3a35",
   "metadata": {},
   "source": [
    "Exercise: **Try out different loss functions for this dataset and plot the values and the fitted line. What do you observe?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a64553",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(loss_functions)):\n",
    "    loss_fn = loss_functions[f]\n",
    "    loss_name = loss_names[f]\n",
    "    model = LinearRegressionGradientDescent(loss_fn, eta=0.001, n_steps=10000)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(X[:,-1], y, color='blue')\n",
    "    plt.plot(X[:,-1], model.predict(X), color='red')\n",
    "    plt.title(f'{loss_name} prediction')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961511f2",
   "metadata": {},
   "source": [
    "Exercise: **Remove the outlier from the dataset. Can you still see differences between MSE, MAE, Huber-loss, $\\epsilon$-insensitive loss and Pinball loss? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7af17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdc4910e",
   "metadata": {},
   "source": [
    "Exercise: **Now create a dataset with two outliers (one high and one low outlier). What does happen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85c156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
